[
  {
    "term": "Core",
    "definition": "An individual processing unit within a CPU or GPU that executes instructions. More cores allow more parallelism."
  },
  {
    "term": "Thread",
    "definition": "The smallest unit of execution scheduled by the CPU. Modern CPUs can run multiple threads per core via Simultaneous Multithreading (SMT), such as Intel Hyper-Threading."
  },
  {
    "term": "Socket",
    "definition": "A physical CPU package slot on the motherboard. Dual-socket systems support two CPUs working together."
  },
  {
    "term": "NUMA (Non-Uniform Memory Access)",
    "definition": "A memory design where each processor has its own local memory but can access other processors’ memory with increased latency. Important in multi-socket systems."
  },
  {
    "term": "vCPU (Virtual CPU)",
    "definition": "A logical CPU core assigned in virtualized or cloud environments, typically mapped to a physical thread."
  },
  {
    "term": "Base Clock",
    "definition": "The guaranteed minimum clock speed (in GHz) at which a CPU operates under standard conditions."
  },
  {
    "term": "Boost Clock / Turbo Frequency",
    "definition": "The maximum clock speed a CPU can reach under high workloads, dynamically managed by thermal and power limits."
  },
  {
    "term": "Clock Speed / Frequency",
    "definition": "The number of cycles a CPU performs per second, measured in GHz. Higher clock speeds typically improve single-threaded performance."
  },
  {
    "term": "IPC (Instructions Per Cycle)",
    "definition": "A measure of how many instructions a CPU can execute in one clock cycle. Critical for understanding real performance beyond raw GHz."
  },
  {
    "term": "Thermal Design Power (TDP)",
    "definition": "The maximum amount of heat generated by a processor under typical load, which dictates the cooling system requirements."
  },
  {
    "term": "FP64 (Double Precision FLOPS)",
    "definition": "Performance metric for 64-bit floating-point calculations. Important in scientific HPC workloads."
  },
  {
    "term": "FP32 (Single Precision FLOPS)",
    "definition": "Performance metric for 32-bit floating-point operations, common in ML/AI and graphics workloads."
  },
  {
    "term": "GFLOPS / TFLOPS",
    "definition": "Giga / Tera Floating Point Operations Per Second – a measure of a CPU/GPU's computational performance."
  },
  {
    "term": "Vector Units / SIMD Units",
    "definition": "Hardware components capable of performing the same operation on multiple data elements simultaneously (Single Instruction Multiple Data)."
  },
  {
    "term": "FMA (Fused Multiply-Add)",
    "definition": "A type of instruction that multiplies two numbers and adds a third in a single step, often used to increase FLOP throughput."
  },
  {
    "term": "Cache (L1/L2/L3)",
    "definition": "Fast memory inside the CPU used to reduce latency in accessing frequently used data. Larger caches generally improve performance."
  },
  {
    "term": "Chiplet Architecture",
    "definition": "A modular CPU design where multiple smaller dies (chiplets) are interconnected to form a single processor. Enables better scalability and yield."
  },
  {
    "term": "Instruction Set Architecture (ISA)",
    "definition": "The low-level instructions a CPU understands, such as x86_64, ARM, or RISC-V."
  },
  {
    "term": "Out-of-Order Execution",
    "definition": "A CPU feature that allows instructions to be executed as resources become available, rather than strictly in program order."
  },
  {
    "term": "Branch Prediction",
    "definition": "A CPU mechanism to guess the direction of conditional branches, reducing pipeline stalls."
  },
  {
    "term": "Memory Speed",
    "definition": "The data transfer rate of RAM, usually measured in MT/s (MegaTransfers per second)."
  },
  {
    "term": "Memory Channels",
    "definition": "Independent pathways between the CPU and memory. More channels = higher memory bandwidth."
  },
  {
    "term": "Memory Bandwidth",
    "definition": "The maximum rate at which data can be read from or written to RAM by the processor, measured in GB/s."
  },
  {
    "term": "HBM (High Bandwidth Memory)",
    "definition": "A type of fast, stacked DRAM used in HPC GPUs and CPUs to provide ultra-high memory bandwidth."
  },
  {
    "term": "ECC Memory (Error-Correcting Code)",
    "definition": "Memory that can detect and correct common types of data corruption, used in mission-critical HPC systems."
  },
  {
    "term": "PCIe Lanes",
    "definition": "High-speed communication lanes used to connect the CPU with GPUs, SSDs, and network cards. More lanes support more or faster devices."
  },
  {
    "term": "Interconnect / UPI / Infinity Fabric",
    "definition": "The links that allow communication between CPU sockets and/or between CPUs and GPUs."
  },
  {
    "term": "Bandwidth Saturation",
    "definition": "A condition where the interconnect or memory bus is fully utilized, potentially becoming a bottleneck."
  },
  {
    "term": "Power Efficiency",
    "definition": "Ratio of performance per watt. Critical in HPC where energy costs and cooling are major factors."
  },
  {
    "term": "AVX Throttling",
    "definition": "Reduction in clock speed when using AVX/AVX-512 instructions due to high power/thermal demand."
  },
  {
    "term": "Die",
    "definition": "A single continuous piece of silicon within a processor package. Multi-die designs allow greater scalability and yield (especially in chiplet-based CPUs)."
  },
  {
    "term": "Multithreading",
    "definition": "A technique allowing multiple threads to run concurrently on a single core, increasing utilization and throughput."
  },
  {
    "term": "Simultaneous Multithreading (SMT)",
    "definition": "An advanced form of multithreading where a core executes instructions from multiple threads in parallel. Intel's implementation is called Hyper-Threading."
  },
  {
    "term": "Execution Unit",
    "definition": "A component within a core responsible for carrying out specific types of instructions (e.g., integer, floating-point, load/store)."
  },
  {
    "term": "Front-End (of CPU)",
    "definition": "The part of the processor that fetches, decodes, and dispatches instructions to execution units. Its efficiency affects overall throughput."
  },
  {
    "term": "Back-End (of CPU)",
    "definition": "The portion responsible for executing instructions and writing back results. Often includes ALUs, FPUs, and memory interfaces."
  },
  {
    "term": "Out-of-Order Window",
    "definition": "The size of the instruction buffer that holds and reorders instructions before execution. Affects ability to exploit instruction-level parallelism."
  },
  {
    "term": "Prefetching",
    "definition": "A technique where the CPU predicts and loads data into cache before it is needed, reducing wait times for memory access."
  },
  {
    "term": "Latency",
    "definition": "The delay between initiating a request and receiving the result. Lower latency is critical for real-time and HPC workloads."
  },
  {
    "term": "Throughput",
    "definition": "The amount of work a system can process in a given time. Different from latency — it reflects bulk processing capability."
  },
  {
    "term": "Pipeline",
    "definition": "A series of stages where instructions are processed. Deep pipelines increase frequency but can suffer from stalls due to mispredictions or data dependencies."
  },
  {
    "term": "Pipeline Stall / Bubble",
    "definition": "A delay in the pipeline flow due to unavailable data or resources, reducing efficiency."
  },
  {
    "term": "Thermal Throttling",
    "definition": "A safety mechanism where the CPU reduces its clock speed to prevent overheating under thermal stress."
  },
  {
    "term": "Die-to-Die Interconnect",
    "definition": "A high-speed communication path between chiplets or dies in a multi-die processor."
  },
  {
    "term": "Fabric Latency",
    "definition": "The delay introduced by the interconnect fabric that links multiple cores, dies, or chips together."
  },
  {
    "term": "Fabric Bandwidth",
    "definition": "The data transfer rate supported by the interconnect fabric, affecting how efficiently processors communicate."
  },
  {
    "term": "Instruction-Level Parallelism (ILP)",
    "definition": "A measure of how many instructions can be executed simultaneously. CPUs exploit ILP to increase efficiency."
  },
  {
    "term": "Data-Level Parallelism (DLP)",
    "definition": "The simultaneous processing of multiple data items using SIMD or GPU-like architectures."
  },
  {
    "term": "Task-Level Parallelism (TLP)",
    "definition": "Running multiple independent tasks in parallel, useful in large-scale multi-core and distributed systems."
  },
  {
    "term": "Microarchitecture",
    "definition": "The internal design of a processor (e.g., Zen 4, Ice Lake) that defines how instructions are executed."
  },
  {
    "term": "Thermal Envelope",
    "definition": "The maximum heat output a system is designed to manage. Exceeding this can cause throttling or shutdowns."
  },
  {
    "term": "Instruction Retirement",
    "definition": "The stage at which an instruction is completed and its results committed to the architectural state."
  },
  {
    "term": "Branch Misprediction Penalty",
    "definition": "The performance cost incurred when the CPU guesses the wrong outcome of a branch and must flush its pipeline."
  },
  {
    "term": "Energy Delay Product (EDP)",
    "definition": "A metric combining power efficiency and performance to evaluate how energy-efficient a processor is under load."
  },
  {
    "term": "Core Affinity",
    "definition": "The assignment of a process or thread to a specific core or socket to reduce migration overhead and improve cache locality."
  },
  {
    "term": "Process Node / Technology Node",
    "definition": "Refers to the manufacturing process used to build chips (e.g., 7nm, 10nm). Smaller nodes generally offer better performance and power efficiency."
  },
  {
    "term": "Micro-Op (μOp)",
    "definition": "A simplified internal instruction derived from complex instructions (macro-ops). CPUs translate complex instructions into μOps for efficient execution."
  },
  {
    "term": "Retirement Rate",
    "definition": "The number of instructions completed and committed per cycle. Key for measuring actual instruction throughput."
  },
  {
    "term": "Superscalar Architecture",
    "definition": "A CPU design that allows multiple instructions to be executed in parallel during each clock cycle."
  },
  {
    "term": "Speculative Execution",
    "definition": "A technique where the CPU guesses the path of execution and runs instructions ahead of time to reduce stalls. Vulnerable to side-channel attacks like Spectre."
  },
  {
    "term": "Register Renaming",
    "definition": "A method to eliminate false data dependencies by mapping logical registers to physical ones dynamically."
  },
  {
    "term": "Reorder Buffer (ROB)",
    "definition": "A buffer that tracks instruction execution and ensures in-order commitment of results for correctness."
  },
  {
    "term": "Reservation Station",
    "definition": "Holds instructions waiting for operands. Once operands are available, instructions are dispatched to execution units."
  },
  {
    "term": "Instruction Fusion",
    "definition": "Combines certain instruction pairs into a single μOp to reduce decoding overhead and improve throughput."
  },
  {
    "term": "Pipeline Depth",
    "definition": "The number of sequential steps an instruction passes through. Deeper pipelines can clock higher but are more vulnerable to stalls."
  },
  {
    "term": "AVX / AVX2 / AVX-512",
    "definition": "Advanced Vector Extensions — SIMD instruction sets used in HPC workloads for high-throughput operations."
  },
  {
    "term": "SSE (Streaming SIMD Extensions)",
    "definition": "Older SIMD instruction sets used for vectorized operations before AVX became standard."
  },
  {
    "term": "Instruction Cache (I-Cache)",
    "definition": "A small, fast memory storing recently fetched instructions. Misses here cause instruction fetch stalls."
  },
  {
    "term": "Data Cache (D-Cache)",
    "definition": "A cache holding recently accessed data values. Efficient data caching is critical in performance-sensitive HPC apps."
  },
  {
    "term": "Cache Miss Penalty",
    "definition": "The performance loss due to fetching data from lower levels of memory when it's not present in the cache."
  },
  {
    "term": "Branch Target Buffer (BTB)",
    "definition": "Predicts target addresses of branch instructions to improve pipeline efficiency."
  },
  {
    "term": "Translation Lookaside Buffer (TLB)",
    "definition": "A cache for virtual-to-physical address translations. Misses can lead to memory access delays."
  },
  {
    "term": "Page Walk",
    "definition": "The process of resolving a virtual memory address to a physical address through multi-level page tables."
  },
  {
    "term": "HyperTransport (HT)",
    "definition": "AMD’s legacy high-speed interconnect technology used for CPU and memory communication (replaced by Infinity Fabric)."
  },
  {
    "term": "Infinity Fabric",
    "definition": "AMD’s scalable interconnect linking cores, dies, and I/O across Ryzen and EPYC CPUs."
  },
  {
    "term": "Ultra Path Interconnect (UPI)",
    "definition": "Intel’s interconnect used in Xeon processors for multi-socket communication, replacing QPI."
  },
  {
    "term": "Topology Awareness",
    "definition": "Optimizing application or OS thread placement based on CPU and memory layout (important in NUMA and SMP systems)."
  },
  {
    "term": "SMT Scaling",
    "definition": "Performance gains achieved when enabling simultaneous multithreading (e.g., 2 threads per core)."
  },
  {
    "term": "Idle Power",
    "definition": "The amount of power a CPU consumes when idle. Important for energy-efficient cluster designs."
  },
  {
    "term": "Load-Store Unit (LSU)",
    "definition": "Handles memory operations like reading (load) and writing (store) between registers and RAM."
  },
  {
    "term": "Arithmetic Logic Unit (ALU)",
    "definition": "Performs integer arithmetic and logic operations. A core may contain multiple ALUs."
  },
  {
    "term": "Floating Point Unit (FPU)",
    "definition": "A dedicated unit for performing floating-point arithmetic. Key in scientific and HPC workloads."
  },
  {
    "term": "Thermal Monitoring Technologies (TMT)",
    "definition": "Hardware features that monitor CPU temperatures and apply thermal protection measures like throttling."
  },
  {
    "term": "Dynamic Voltage and Frequency Scaling (DVFS)",
    "definition": "Adjusts CPU voltage and frequency based on current workload to balance performance and power."
  },
  {
    "term": "Turbo Boost Technology (Intel) / Precision Boost (AMD)",
    "definition": "Automatic frequency scaling mechanisms that temporarily increase clock speeds under load."
  },
  {
    "term": "Secure Enclaves / SGX / SEV",
    "definition": "Hardware-based trusted execution environments (Intel SGX, AMD SEV) that isolate sensitive computations, sometimes relevant in secure HPC."
  },
  {
    "term": "Processor Stepping",
    "definition": "Minor revisions of a processor model, often with silicon-level bug fixes or optimizations. Stepping can affect performance or compatibility."
  },
  {
    "term": "Die Shrink",
    "definition": "A reduction in manufacturing process size (e.g., 10nm to 7nm) that improves performance, efficiency, and transistor density."
  },
  {
    "term": "Core Complex (CCX)",
    "definition": "AMD architecture unit consisting of a group of CPU cores sharing some cache and interconnects."
  },
  {
    "term": "Node Interleaving",
    "definition": "BIOS setting that spreads memory allocation across all NUMA nodes to improve bandwidth at the cost of latency."
  }
]