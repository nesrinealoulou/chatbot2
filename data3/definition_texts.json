[
  "Core: An individual processing unit within a CPU or GPU that executes instructions. More cores allow more parallelism.",
  "Thread: The smallest unit of execution scheduled by the CPU. Modern CPUs can run multiple threads per core via Simultaneous Multithreading (SMT), such as Intel Hyper-Threading.",
  "Socket: A physical CPU package slot on the motherboard. Dual-socket systems support two CPUs working together.",
  "NUMA (Non-Uniform Memory Access): A memory design where each processor has its own local memory but can access other processors’ memory with increased latency. Important in multi-socket systems.",
  "vCPU (Virtual CPU): A logical CPU core assigned in virtualized or cloud environments, typically mapped to a physical thread.",
  "Base Clock: The guaranteed minimum clock speed (in GHz) at which a CPU operates under standard conditions.",
  "Boost Clock / Turbo Frequency: The maximum clock speed a CPU can reach under high workloads, dynamically managed by thermal and power limits.",
  "Clock Speed / Frequency: The number of cycles a CPU performs per second, measured in GHz. Higher clock speeds typically improve single-threaded performance.",
  "IPC (Instructions Per Cycle): A measure of how many instructions a CPU can execute in one clock cycle. Critical for understanding real performance beyond raw GHz.",
  "Thermal Design Power (TDP): The maximum amount of heat generated by a processor under typical load, which dictates the cooling system requirements.",
  "FP64 (Double Precision FLOPS): Performance metric for 64-bit floating-point calculations. Important in scientific HPC workloads.",
  "FP32 (Single Precision FLOPS): Performance metric for 32-bit floating-point operations, common in ML/AI and graphics workloads.",
  "GFLOPS / TFLOPS: Giga / Tera Floating Point Operations Per Second – a measure of a CPU/GPU's computational performance.",
  "Vector Units / SIMD Units: Hardware components capable of performing the same operation on multiple data elements simultaneously (Single Instruction Multiple Data).",
  "FMA (Fused Multiply-Add): A type of instruction that multiplies two numbers and adds a third in a single step, often used to increase FLOP throughput.",
  "Cache (L1/L2/L3): Fast memory inside the CPU used to reduce latency in accessing frequently used data. Larger caches generally improve performance.",
  "Chiplet Architecture: A modular CPU design where multiple smaller dies (chiplets) are interconnected to form a single processor. Enables better scalability and yield.",
  "Instruction Set Architecture (ISA): The low-level instructions a CPU understands, such as x86_64, ARM, or RISC-V.",
  "Out-of-Order Execution: A CPU feature that allows instructions to be executed as resources become available, rather than strictly in program order.",
  "Branch Prediction: A CPU mechanism to guess the direction of conditional branches, reducing pipeline stalls.",
  "Memory Speed: The data transfer rate of RAM, usually measured in MT/s (MegaTransfers per second).",
  "Memory Channels: Independent pathways between the CPU and memory. More channels = higher memory bandwidth.",
  "Memory Bandwidth: The maximum rate at which data can be read from or written to RAM by the processor, measured in GB/s.",
  "HBM (High Bandwidth Memory): A type of fast, stacked DRAM used in HPC GPUs and CPUs to provide ultra-high memory bandwidth.",
  "ECC Memory (Error-Correcting Code): Memory that can detect and correct common types of data corruption, used in mission-critical HPC systems.",
  "PCIe Lanes: High-speed communication lanes used to connect the CPU with GPUs, SSDs, and network cards. More lanes support more or faster devices.",
  "Interconnect / UPI / Infinity Fabric: The links that allow communication between CPU sockets and/or between CPUs and GPUs.",
  "Bandwidth Saturation: A condition where the interconnect or memory bus is fully utilized, potentially becoming a bottleneck.",
  "Power Efficiency: Ratio of performance per watt. Critical in HPC where energy costs and cooling are major factors.",
  "AVX Throttling: Reduction in clock speed when using AVX/AVX-512 instructions due to high power/thermal demand.",
  "Die: A single continuous piece of silicon within a processor package. Multi-die designs allow greater scalability and yield (especially in chiplet-based CPUs).",
  "Multithreading: A technique allowing multiple threads to run concurrently on a single core, increasing utilization and throughput.",
  "Simultaneous Multithreading (SMT): An advanced form of multithreading where a core executes instructions from multiple threads in parallel. Intel's implementation is called Hyper-Threading.",
  "Execution Unit: A component within a core responsible for carrying out specific types of instructions (e.g., integer, floating-point, load/store).",
  "Front-End (of CPU): The part of the processor that fetches, decodes, and dispatches instructions to execution units. Its efficiency affects overall throughput.",
  "Back-End (of CPU): The portion responsible for executing instructions and writing back results. Often includes ALUs, FPUs, and memory interfaces.",
  "Out-of-Order Window: The size of the instruction buffer that holds and reorders instructions before execution. Affects ability to exploit instruction-level parallelism.",
  "Prefetching: A technique where the CPU predicts and loads data into cache before it is needed, reducing wait times for memory access.",
  "Latency: The delay between initiating a request and receiving the result. Lower latency is critical for real-time and HPC workloads.",
  "Throughput: The amount of work a system can process in a given time. Different from latency — it reflects bulk processing capability.",
  "Pipeline: A series of stages where instructions are processed. Deep pipelines increase frequency but can suffer from stalls due to mispredictions or data dependencies.",
  "Pipeline Stall / Bubble: A delay in the pipeline flow due to unavailable data or resources, reducing efficiency.",
  "Thermal Throttling: A safety mechanism where the CPU reduces its clock speed to prevent overheating under thermal stress.",
  "Die-to-Die Interconnect: A high-speed communication path between chiplets or dies in a multi-die processor.",
  "Fabric Latency: The delay introduced by the interconnect fabric that links multiple cores, dies, or chips together.",
  "Fabric Bandwidth: The data transfer rate supported by the interconnect fabric, affecting how efficiently processors communicate.",
  "Instruction-Level Parallelism (ILP): A measure of how many instructions can be executed simultaneously. CPUs exploit ILP to increase efficiency.",
  "Data-Level Parallelism (DLP): The simultaneous processing of multiple data items using SIMD or GPU-like architectures.",
  "Task-Level Parallelism (TLP): Running multiple independent tasks in parallel, useful in large-scale multi-core and distributed systems.",
  "Microarchitecture: The internal design of a processor (e.g., Zen 4, Ice Lake) that defines how instructions are executed.",
  "Thermal Envelope: The maximum heat output a system is designed to manage. Exceeding this can cause throttling or shutdowns.",
  "Instruction Retirement: The stage at which an instruction is completed and its results committed to the architectural state.",
  "Branch Misprediction Penalty: The performance cost incurred when the CPU guesses the wrong outcome of a branch and must flush its pipeline.",
  "Energy Delay Product (EDP): A metric combining power efficiency and performance to evaluate how energy-efficient a processor is under load.",
  "Core Affinity: The assignment of a process or thread to a specific core or socket to reduce migration overhead and improve cache locality.",
  "Process Node / Technology Node: Refers to the manufacturing process used to build chips (e.g., 7nm, 10nm). Smaller nodes generally offer better performance and power efficiency.",
  "Micro-Op (μOp): A simplified internal instruction derived from complex instructions (macro-ops). CPUs translate complex instructions into μOps for efficient execution.",
  "Retirement Rate: The number of instructions completed and committed per cycle. Key for measuring actual instruction throughput.",
  "Superscalar Architecture: A CPU design that allows multiple instructions to be executed in parallel during each clock cycle.",
  "Speculative Execution: A technique where the CPU guesses the path of execution and runs instructions ahead of time to reduce stalls. Vulnerable to side-channel attacks like Spectre.",
  "Register Renaming: A method to eliminate false data dependencies by mapping logical registers to physical ones dynamically.",
  "Reorder Buffer (ROB): A buffer that tracks instruction execution and ensures in-order commitment of results for correctness.",
  "Reservation Station: Holds instructions waiting for operands. Once operands are available, instructions are dispatched to execution units.",
  "Instruction Fusion: Combines certain instruction pairs into a single μOp to reduce decoding overhead and improve throughput.",
  "Pipeline Depth: The number of sequential steps an instruction passes through. Deeper pipelines can clock higher but are more vulnerable to stalls.",
  "AVX / AVX2 / AVX-512: Advanced Vector Extensions — SIMD instruction sets used in HPC workloads for high-throughput operations.",
  "SSE (Streaming SIMD Extensions): Older SIMD instruction sets used for vectorized operations before AVX became standard.",
  "Instruction Cache (I-Cache): A small, fast memory storing recently fetched instructions. Misses here cause instruction fetch stalls.",
  "Data Cache (D-Cache): A cache holding recently accessed data values. Efficient data caching is critical in performance-sensitive HPC apps.",
  "Cache Miss Penalty: The performance loss due to fetching data from lower levels of memory when it's not present in the cache.",
  "Branch Target Buffer (BTB): Predicts target addresses of branch instructions to improve pipeline efficiency.",
  "Translation Lookaside Buffer (TLB): A cache for virtual-to-physical address translations. Misses can lead to memory access delays.",
  "Page Walk: The process of resolving a virtual memory address to a physical address through multi-level page tables.",
  "HyperTransport (HT): AMD’s legacy high-speed interconnect technology used for CPU and memory communication (replaced by Infinity Fabric).",
  "Infinity Fabric: AMD’s scalable interconnect linking cores, dies, and I/O across Ryzen and EPYC CPUs.",
  "Ultra Path Interconnect (UPI): Intel’s interconnect used in Xeon processors for multi-socket communication, replacing QPI.",
  "Topology Awareness: Optimizing application or OS thread placement based on CPU and memory layout (important in NUMA and SMP systems).",
  "SMT Scaling: Performance gains achieved when enabling simultaneous multithreading (e.g., 2 threads per core).",
  "Idle Power: The amount of power a CPU consumes when idle. Important for energy-efficient cluster designs.",
  "Load-Store Unit (LSU): Handles memory operations like reading (load) and writing (store) between registers and RAM.",
  "Arithmetic Logic Unit (ALU): Performs integer arithmetic and logic operations. A core may contain multiple ALUs.",
  "Floating Point Unit (FPU): A dedicated unit for performing floating-point arithmetic. Key in scientific and HPC workloads.",
  "Thermal Monitoring Technologies (TMT): Hardware features that monitor CPU temperatures and apply thermal protection measures like throttling.",
  "Dynamic Voltage and Frequency Scaling (DVFS): Adjusts CPU voltage and frequency based on current workload to balance performance and power.",
  "Turbo Boost Technology (Intel) / Precision Boost (AMD): Automatic frequency scaling mechanisms that temporarily increase clock speeds under load.",
  "Secure Enclaves / SGX / SEV: Hardware-based trusted execution environments (Intel SGX, AMD SEV) that isolate sensitive computations, sometimes relevant in secure HPC.",
  "Processor Stepping: Minor revisions of a processor model, often with silicon-level bug fixes or optimizations. Stepping can affect performance or compatibility.",
  "Die Shrink: A reduction in manufacturing process size (e.g., 10nm to 7nm) that improves performance, efficiency, and transistor density.",
  "Core Complex (CCX): AMD architecture unit consisting of a group of CPU cores sharing some cache and interconnects.",
  "Node Interleaving: BIOS setting that spreads memory allocation across all NUMA nodes to improve bandwidth at the cost of latency."
]